import torch
import torch.nn as nn


class Mlp(nn.Module):
    """
    Multilayer Perceptron
    Parameters:
    -----------
    in_features: int
                 Size of input
    hidden_features:  int, optional
                      Size of hidden layer
    out_features: int, optional
                  Size of output
    act_layer: nn.Module, optional
               Activation function
    drop_rate: float, optional
               Dropout rate
    """
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop_rate=0.0,
    ):
        super().__init__()
        self.drop_rate = drop_rate
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        if self.drop_rate > 0.0:
            self.drop = nn.Dropout(drop_rate)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        if self.drop_rate > 0.0:
            x = self.drop(x)
        x = self.fc2(x)
        if self.drop_rate > 0.0:
            x = self.drop(x)
        return x
